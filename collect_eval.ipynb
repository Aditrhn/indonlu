{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the noted finetuned eval data\n",
    "df_wwmlm_1e5 = pd.DataFrame({'task_name':['absa-airy', 'emotion-twitter', 'doc-sentiment-prosa', 'keyword-extraction-prosa', 'ner-grit', 'pos-idn', 'term-extraction-airy', 'entailment-ui', 'pos-prosa', 'ner-prosa'],\n",
    "                              'F1':[0.7085, 0.7344, 0.8892, 0.6221, 0.7385, 0.9478, 0.8911, 0.7711, 0.964415, 0.789788]})\n",
    "df_wwmlm_1e5['exp_name'] = 'babert_model_bpe_wwmlm_b8_step1_gamma0.8_lr1e-5_early10'\n",
    "df_wwmlm_1e5['stats'] = 'mean'\n",
    "\n",
    "df_wwmlm_625e6 = pd.DataFrame({'task_name':['absa-airy', 'emotion-twitter', 'doc-sentiment-prosa', 'keyword-extraction-prosa', 'ner-grit', 'pos-idn', 'term-extraction-airy', 'entailment-ui', 'pos-prosa', 'ner-prosa'],\n",
    "                              'F1':[0.6781, 0.7233, 0.8602, 0.6447, 0.7483, 0.9602, 0.9039, 0.8219, 0.959804, 0.800779]})\n",
    "df_wwmlm_625e6['exp_name'] = 'babert_model_bpe_wwmlm_b8_step1_gamma0.8_lr6.25e-6_early10'\n",
    "df_wwmlm_625e6['stats'] = 'mean'\n",
    "\n",
    "xlmr_base = pd.DataFrame({'task_name':['absa-airy', 'emotion-twitter', 'doc-sentiment-prosa', 'keyword-extraction-prosa', 'ner-grit', 'ner-prosa', 'pos-prosa', 'pos-idn', 'term-extraction-airy', 'entailment-ui'],\n",
    "                            'F1':[0.7126, 0.7251, 0.9229, 0.6206, 0.8011, 0.7967, 0.9667, 0.9672, 0.8984, 0.8278]})\n",
    "xlmr_base['exp_name'] = 'xlmr_base'\n",
    "xlmr_base['stats'] = 'mean'\n",
    "\n",
    "babert_bpe_mlm = pd.DataFrame({'task_name':['absa-airy', 'emotion-twitter', 'doc-sentiment-prosa', 'keyword-extraction-prosa', 'ner-grit', 'ner-prosa', 'pos-prosa', 'pos-idn', 'term-extraction-airy', 'entailment-ui'],\n",
    "                                'F1':[0.7287, 0.7559, 0.8981, 0.6558, 0.7582, 0.8180, 0.9692, 0.9624, 0.9020, 0.8325]})\n",
    "babert_bpe_mlm['exp_name'] = 'babert_bpe_mlm'\n",
    "babert_bpe_mlm['stats'] = 'mean'\n",
    "\n",
    "xlm_mlm_large = pd.DataFrame({'task_name':['absa-airy', 'emotion-twitter', 'doc-sentiment-prosa', 'keyword-extraction-prosa', 'ner-grit', 'ner-prosa', 'pos-prosa', 'pos-idn', 'term-extraction-airy', 'entailment-ui'],\n",
    "                                'F1':[0.5849, 0.6546, 0.8567, 0.5703, 0.8138, 0.8169, 0.9677, 0.9685, 0, 0.6500]})\n",
    "xlm_mlm_large['exp_name'] = 'xlm_mlm_large'\n",
    "xlm_mlm_large['stats'] = 'mean'\n",
    "\n",
    "xlmr_large = pd.DataFrame({'task_name':['absa-airy', 'emotion-twitter', 'doc-sentiment-prosa', 'keyword-extraction-prosa', 'ner-grit', 'ner-prosa', 'pos-prosa', 'pos-idn', 'term-extraction-airy', 'entailment-ui'],\n",
    "                                'F1':[0.7500, 0.7677, 0.9335, 0.6092, 0.7939, 0, 0.0794, 0.9689, 0.9087, 0.8728]})\n",
    "xlmr_large['exp_name'] = 'xlmr_large'\n",
    "xlmr_large['stats'] = 'mean'\n",
    "\n",
    "# take the recently finetuned eval data\n",
    "base_path = './save'\n",
    "dfs = []\n",
    "for task_path in os.listdir(base_path):\n",
    "    for exp_path in os.listdir(f'{base_path}/{task_path}'):\n",
    "        if os.path.exists(f'{base_path}/{task_path}/{exp_path}/evaluation_result.csv'):\n",
    "            df = pd.read_csv(f'{base_path}/{task_path}/{exp_path}/evaluation_result.csv')\n",
    "            df.columns = ['stats'] + df.columns[1:].tolist()\n",
    "            df['task_name'] = task_path\n",
    "            df['exp_name'] = exp_path\n",
    "            dfs.append(df.loc[df['stats'] == 'mean',:])\n",
    "combined_df = pd.concat(dfs + [df_wwmlm_1e5, df_wwmlm_625e6, xlmr_base, babert_bpe_mlm, xlm_mlm_large, xlmr_large], sort=False)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for metric in ['ACC', 'F1', 'REC', 'PRE']:\n",
    "    combined_df[metric] = combined_df[metric].apply(lambda x: x / 100 if x > 1 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Each Task's Standings vs NoN Large Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th>task_name</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>xlmr_base</td>\n",
       "      <td>doc-sentiment-prosa</td>\n",
       "      <td>0.9229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>xlmr_base</td>\n",
       "      <td>ner-grit</td>\n",
       "      <td>0.8011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>xlmr_base</td>\n",
       "      <td>pos-idn</td>\n",
       "      <td>0.9672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      exp_name            task_name      F1\n",
       "213  xlmr_base  doc-sentiment-prosa  0.9229\n",
       "215  xlmr_base  ner-grit             0.8011\n",
       "218  xlmr_base  pos-idn              0.9672"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzed_df = combined_df[~combined_df['exp_name'].isin(['xlmr_large', 'xlm_mlm_large'])]\n",
    "maxes = analyzed_df.groupby('task_name')['F1'].idxmax()\n",
    "combined_df_maxes = analyzed_df.loc[maxes][['exp_name', 'task_name', 'F1']]\n",
    "undefeated_list = ['doc-sentiment-prosa', 'pos-idn', 'ner-grit']\n",
    "combined_df_maxes[combined_df_maxes['task_name'].isin(undefeated_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th>task_name</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>babert_model_word_mlm_b8_step1_gamma0.8_lr1e-5_early10</td>\n",
       "      <td>absa-airy</td>\n",
       "      <td>0.731188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>babert_bpe_mlm</td>\n",
       "      <td>emotion-twitter</td>\n",
       "      <td>0.755900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>babert_bpe_mlm</td>\n",
       "      <td>entailment-ui</td>\n",
       "      <td>0.832500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>babert_bpe_mlm</td>\n",
       "      <td>keyword-extraction-prosa</td>\n",
       "      <td>0.655800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>ner-prosa</td>\n",
       "      <td>0.831198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>babert_bpe_mlm</td>\n",
       "      <td>pos-prosa</td>\n",
       "      <td>0.969200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>babert_model_bpe_wwmlm_ckpt_2000000_b8_step1_gamma0.8_lr1e-5_early10</td>\n",
       "      <td>qa-factoid-itb</td>\n",
       "      <td>0.442857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>term-extraction-airy</td>\n",
       "      <td>0.905271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        exp_name  \\\n",
       "121  babert_model_word_mlm_b8_step1_gamma0.8_lr1e-5_early10                        \n",
       "222  babert_bpe_mlm                                                                \n",
       "230  babert_bpe_mlm                                                                \n",
       "224  babert_bpe_mlm                                                                \n",
       "163  babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr1e-5_early5   \n",
       "227  babert_bpe_mlm                                                                \n",
       "21   babert_model_bpe_wwmlm_ckpt_2000000_b8_step1_gamma0.8_lr1e-5_early10          \n",
       "57   babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr1e-5_early5   \n",
       "\n",
       "                    task_name        F1  \n",
       "121  absa-airy                 0.731188  \n",
       "222  emotion-twitter           0.755900  \n",
       "230  entailment-ui             0.832500  \n",
       "224  keyword-extraction-prosa  0.655800  \n",
       "163  ner-prosa                 0.831198  \n",
       "227  pos-prosa                 0.969200  \n",
       "21   qa-factoid-itb            0.442857  \n",
       "57   term-extraction-airy      0.905271  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df_maxes[~combined_df_maxes['task_name'].isin(undefeated_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If versus large model, we haven't yet defeated:\n",
    "\n",
    "- __absa-airy__: __75__\n",
    "- __doc-sentiment-prosa__: __93.35 or 92.29__\n",
    "- __emotion-twitter__: __76.77__ (BASEnya kita udah menang)\n",
    "- __entailment-ui__: __87.28__ (BASEnya kita udah menang) -> cepet finetuningnya\n",
    "- __ner-grit__: __81.38__\n",
    "- __pos-idn__: __96.89 or 96.72__\n",
    "- __term-extraction-airy__: __90.87__ (BASEnya kita 90.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndoNLU Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th colspan=\"2\" halign=\"left\">F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babert_bpe_mlm</td>\n",
       "      <td>0.828080</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xlmr_base</td>\n",
       "      <td>0.823910</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr1e-5_early10</td>\n",
       "      <td>0.818826</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.816919</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr6.25e-6_early10</td>\n",
       "      <td>0.813624</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>babert_model_bpe_wwmlm_ckpt_2000000_b8_step1_gamma0.8_lr1e-5_early10</td>\n",
       "      <td>0.811920</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>babert_model_bpe_wwmlm_ckpt_2000000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.811793</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>babert_model_bpe_wwmlm_b8_step1_gamma0.8_lr6.25e-6_early10</td>\n",
       "      <td>0.810118</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>babert_model_bpe_wwmlm_b8_step1_gamma0.8_lr1e-5_early10</td>\n",
       "      <td>0.805690</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>babert_model_word_mlm_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.796764</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>babert_model_word_mlm_b8_step1_gamma0.8_lr1e-5_early10</td>\n",
       "      <td>0.796764</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>babert_model_unigram_mlm_b8_step1_gamma0.8_lr1e-5_early10</td>\n",
       "      <td>0.796192</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>babert_model_unigram_mlm_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.795271</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>babert_model_word_mlm_512_ckpt_1150000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.793463</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>babert_model_unigram_mlm_512_ckpt_160000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.791164</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>babert_model_bpe_mlm_lr_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.789484</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>babert_model_unigram_mlm_512_ckpt_132500_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.788199</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>babert_model_bpe_512_full_lr2e5_ckpt_220000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.779104</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>babert_model_bpe_512_full_lr2e5_ckpt_185000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.778346</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>babert_model_unigram_mlm_small_bs_ckpt_160000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.776894</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>babert_model_unigram_mlm_small_bs_ckpt_130000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.768803</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>xlm_mlm_large</td>\n",
       "      <td>0.688340</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>xlmr_large</td>\n",
       "      <td>0.668410</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           exp_name  \\\n",
       "                                                                                      \n",
       "0   babert_bpe_mlm                                                                    \n",
       "1   xlmr_base                                                                         \n",
       "2   babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr1e-5_early10      \n",
       "3   babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr1e-5_early5       \n",
       "4   babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr6.25e-6_early10   \n",
       "5   babert_model_bpe_wwmlm_ckpt_2000000_b8_step1_gamma0.8_lr1e-5_early10              \n",
       "6   babert_model_bpe_wwmlm_ckpt_2000000_b8_step1_gamma0.8_lr1e-5_early5               \n",
       "7   babert_model_bpe_wwmlm_b8_step1_gamma0.8_lr6.25e-6_early10                        \n",
       "8   babert_model_bpe_wwmlm_b8_step1_gamma0.8_lr1e-5_early10                           \n",
       "9   babert_model_word_mlm_b8_step1_gamma0.8_lr1e-5_early5                             \n",
       "10  babert_model_word_mlm_b8_step1_gamma0.8_lr1e-5_early10                            \n",
       "11  babert_model_unigram_mlm_b8_step1_gamma0.8_lr1e-5_early10                         \n",
       "12  babert_model_unigram_mlm_b8_step1_gamma0.8_lr1e-5_early5                          \n",
       "13  babert_model_word_mlm_512_ckpt_1150000_b8_step1_gamma0.8_lr1e-5_early5            \n",
       "14  babert_model_unigram_mlm_512_ckpt_160000_b8_step1_gamma0.8_lr1e-5_early5          \n",
       "15  babert_model_bpe_mlm_lr_b8_step1_gamma0.8_lr1e-5_early5                           \n",
       "16  babert_model_unigram_mlm_512_ckpt_132500_b8_step1_gamma0.8_lr1e-5_early5          \n",
       "17  babert_model_bpe_512_full_lr2e5_ckpt_220000_b8_step1_gamma0.8_lr1e-5_early5       \n",
       "18  babert_model_bpe_512_full_lr2e5_ckpt_185000_b8_step1_gamma0.8_lr1e-5_early5       \n",
       "19  babert_model_unigram_mlm_small_bs_ckpt_160000_b8_step1_gamma0.8_lr1e-5_early5     \n",
       "20  babert_model_unigram_mlm_small_bs_ckpt_130000_b8_step1_gamma0.8_lr1e-5_early5     \n",
       "21  xlm_mlm_large                                                                     \n",
       "22  xlmr_large                                                                        \n",
       "\n",
       "          F1        \n",
       "        mean count  \n",
       "0   0.828080  10    \n",
       "1   0.823910  10    \n",
       "2   0.818826  10    \n",
       "3   0.816919  10    \n",
       "4   0.813624  10    \n",
       "5   0.811920  10    \n",
       "6   0.811793  10    \n",
       "7   0.810118  10    \n",
       "8   0.805690  10    \n",
       "9   0.796764  10    \n",
       "10  0.796764  10    \n",
       "11  0.796192  10    \n",
       "12  0.795271  10    \n",
       "13  0.793463  10    \n",
       "14  0.791164  10    \n",
       "15  0.789484  10    \n",
       "16  0.788199  10    \n",
       "17  0.779104  10    \n",
       "18  0.778346  10    \n",
       "19  0.776894  10    \n",
       "20  0.768803  10    \n",
       "21  0.688340  10    \n",
       "22  0.668410  10    "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indonlu_eval = combined_df[~combined_df['task_name'].isin(['qa-factoid-itb'])]\n",
    "indonlu_eval_grouped = indonlu_eval.groupby('exp_name').agg(['mean', 'count'])\n",
    "indonlu_eval_grouped[indonlu_eval_grouped[('F1','count')]==10][['F1']].reset_index().sort_values(by=('F1','mean'), ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th colspan=\"2\" halign=\"left\">F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babert_bpe_mlm</td>\n",
       "      <td>0.801200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>xlmr_base</td>\n",
       "      <td>0.796750</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr1e-5_early10</td>\n",
       "      <td>0.789828</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.787445</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr6.25e-6_early10</td>\n",
       "      <td>0.784566</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>babert_model_bpe_wwmlm_ckpt_2000000_b8_step1_gamma0.8_lr1e-5_early10</td>\n",
       "      <td>0.781867</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>babert_model_bpe_wwmlm_ckpt_2000000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.781867</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>babert_model_bpe_wwmlm_b8_step1_gamma0.8_lr6.25e-6_early10</td>\n",
       "      <td>0.779685</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>babert_model_bpe_wwmlm_b8_step1_gamma0.8_lr1e-5_early10</td>\n",
       "      <td>0.775173</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>babert_model_word_mlm_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.766666</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>babert_model_word_mlm_b8_step1_gamma0.8_lr1e-5_early10</td>\n",
       "      <td>0.766666</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>babert_model_unigram_mlm_b8_step1_gamma0.8_lr1e-5_early10</td>\n",
       "      <td>0.764182</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>babert_model_unigram_mlm_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.763030</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>babert_model_word_mlm_512_ckpt_1150000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.762454</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>babert_model_unigram_mlm_512_ckpt_160000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>babert_model_unigram_mlm_512_ckpt_132500_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.754895</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>babert_model_bpe_mlm_lr_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.754626</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>babert_model_bpe_512_full_lr2e5_ckpt_220000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.743777</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>babert_model_bpe_512_full_lr2e5_ckpt_185000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.742356</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>babert_model_unigram_mlm_small_bs_ckpt_160000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.740677</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xlm_mlm_large</td>\n",
       "      <td>0.739463</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>babert_model_unigram_mlm_small_bs_ckpt_130000_b8_step1_gamma0.8_lr1e-5_early5</td>\n",
       "      <td>0.729813</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>xlmr_large</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           exp_name  \\\n",
       "                                                                                      \n",
       "0   babert_bpe_mlm                                                                    \n",
       "21  xlmr_base                                                                         \n",
       "4   babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr1e-5_early10      \n",
       "5   babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr1e-5_early5       \n",
       "6   babert_model_bpe_wwmlm_128_256_ckpt_1000000_b8_step1_gamma0.8_lr6.25e-6_early10   \n",
       "9   babert_model_bpe_wwmlm_ckpt_2000000_b8_step1_gamma0.8_lr1e-5_early10              \n",
       "10  babert_model_bpe_wwmlm_ckpt_2000000_b8_step1_gamma0.8_lr1e-5_early5               \n",
       "8   babert_model_bpe_wwmlm_b8_step1_gamma0.8_lr6.25e-6_early10                        \n",
       "7   babert_model_bpe_wwmlm_b8_step1_gamma0.8_lr1e-5_early10                           \n",
       "19  babert_model_word_mlm_b8_step1_gamma0.8_lr1e-5_early5                             \n",
       "18  babert_model_word_mlm_b8_step1_gamma0.8_lr1e-5_early10                            \n",
       "13  babert_model_unigram_mlm_b8_step1_gamma0.8_lr1e-5_early10                         \n",
       "14  babert_model_unigram_mlm_b8_step1_gamma0.8_lr1e-5_early5                          \n",
       "17  babert_model_word_mlm_512_ckpt_1150000_b8_step1_gamma0.8_lr1e-5_early5            \n",
       "12  babert_model_unigram_mlm_512_ckpt_160000_b8_step1_gamma0.8_lr1e-5_early5          \n",
       "11  babert_model_unigram_mlm_512_ckpt_132500_b8_step1_gamma0.8_lr1e-5_early5          \n",
       "3   babert_model_bpe_mlm_lr_b8_step1_gamma0.8_lr1e-5_early5                           \n",
       "2   babert_model_bpe_512_full_lr2e5_ckpt_220000_b8_step1_gamma0.8_lr1e-5_early5       \n",
       "1   babert_model_bpe_512_full_lr2e5_ckpt_185000_b8_step1_gamma0.8_lr1e-5_early5       \n",
       "16  babert_model_unigram_mlm_small_bs_ckpt_160000_b8_step1_gamma0.8_lr1e-5_early5     \n",
       "20  xlm_mlm_large                                                                     \n",
       "15  babert_model_unigram_mlm_small_bs_ckpt_130000_b8_step1_gamma0.8_lr1e-5_early5     \n",
       "22  xlmr_large                                                                        \n",
       "\n",
       "          F1        \n",
       "        mean count  \n",
       "0   0.801200  8     \n",
       "21  0.796750  8     \n",
       "4   0.789828  8     \n",
       "5   0.787445  8     \n",
       "6   0.784566  8     \n",
       "9   0.781867  8     \n",
       "10  0.781867  8     \n",
       "8   0.779685  8     \n",
       "7   0.775173  8     \n",
       "19  0.766666  8     \n",
       "18  0.766666  8     \n",
       "13  0.764182  8     \n",
       "14  0.763030  8     \n",
       "17  0.762454  8     \n",
       "12  0.758065  8     \n",
       "11  0.754895  8     \n",
       "3   0.754626  8     \n",
       "2   0.743777  8     \n",
       "1   0.742356  8     \n",
       "16  0.740677  8     \n",
       "20  0.739463  8     \n",
       "15  0.729813  8     \n",
       "22  0.712000  8     "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indonlu_eval = combined_df[~combined_df['task_name'].isin(['qa-factoid-itb', 'term-extraction-airy', 'pos-prosa'])]\n",
    "indonlu_eval_grouped = indonlu_eval.groupby('exp_name').agg(['mean', 'count'])\n",
    "indonlu_eval_grouped[indonlu_eval_grouped[('F1','count')]==8][['F1']].reset_index().sort_values(by=('F1','mean'), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_model_list = [\n",
    "    'scratch',\n",
    "    'word2vec_',\n",
    "    'fasttext_',\n",
    "    'fasttext-cc-id',\n",
    "    'fasttext-cc-id-no-oov',\n",
    "    'babert-opensubtitle',\n",
    "    'babert-opensubtitle',\n",
    "    'bert-base-multilingual-uncased',\n",
    "    'xlm-roberta-base'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_to_index(exp_name):\n",
    "    for i, model in enumerate(sorted_model_list):\n",
    "        if model in exp_name:\n",
    "            return i\n",
    "combined_df['model_index'] = combined_df['exp_name'].apply(lambda x: exp_to_index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.sort_values(['task_name', 'model_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('aggregated_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
